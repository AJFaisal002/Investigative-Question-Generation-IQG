{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12443031,"sourceType":"datasetVersion","datasetId":7849167}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:44:00.807507Z","iopub.execute_input":"2025-08-02T15:44:00.807767Z","iopub.status.idle":"2025-08-02T15:44:01.125922Z","shell.execute_reply.started":"2025-08-02T15:44:00.807746Z","shell.execute_reply":"2025-08-02T15:44:01.125231Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/trec-2025/2025-starter-kit-main/.example.env\n/kaggle/input/trec-2025/2025-starter-kit-main/main.py\n/kaggle/input/trec-2025/2025-starter-kit-main/README.md\n/kaggle/input/trec-2025/2025-starter-kit-main/produce_run.py\n/kaggle/input/trec-2025/2025-starter-kit-main/output/tracking_data.json\n/kaggle/input/trec-2025/2025-starter-kit-main/output/dragun-organizers-starter-kit-task-1\n/kaggle/input/trec-2025/2025-starter-kit-main/output/dragun-organizers-starter-kit-task-2\n/kaggle/input/trec-2025/2025-starter-kit-main/data/trec-2025-dragun-topics.jsonl\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/report_generator.py\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/segment_retriever.py\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/question_generator.py\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/query_generator.py\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/information_evaluator.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n    !pip install transformers==4.51.3\n    !pip install --no-deps unsloth\n    !pip install langchain langchain-community\n    !pip install jsonlines\n    !pip install --quiet langchain transformers sentencepiece\n\nimport pandas as pd\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import LLMChain\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nimport json\nfrom unsloth import FastLanguageModel\nfrom unsloth import FastModel\nimport torch\nfrom tqdm import tqdm\nfrom unsloth.chat_templates import get_chat_template","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:44:01.127352Z","iopub.execute_input":"2025-08-02T15:44:01.127684Z","iopub.status.idle":"2025-08-02T15:45:18.675753Z","shell.execute_reply.started":"2025-08-02T15:44:01.127665Z","shell.execute_reply":"2025-08-02T15:45:18.674970Z"}},"outputs":[{"name":"stderr","text":"2025-08-02 15:44:50.424514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754149490.656545      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754149490.729049      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"TOPICS_PATH = '/kaggle/input/trec-2025/2025-starter-kit-main/data/trec-2025-dragun-topics.jsonl'\n\ndef load_jsonl(fname):\n    with open(fname, 'r', encoding='utf-8') as f:\n        return [json.loads(line) for line in f]\ntopics = load_jsonl(TOPICS_PATH)\nprint(\"Loaded\", len(topics), \"topics.\")\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)  \n\nfrom langchain.prompts import PromptTemplate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:45:18.676599Z","iopub.execute_input":"2025-08-02T15:45:18.676846Z","iopub.status.idle":"2025-08-02T15:46:09.999431Z","shell.execute_reply.started":"2025-08-02T15:45:18.676811Z","shell.execute_reply":"2025-08-02T15:46:09.998559Z"}},"outputs":[{"name":"stdout","text":"Loaded 30 topics.\n==((====))==  Unsloth 2025.8.1: Fast Qwen3 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83097e9e158543e9a1dbe6a1b13df479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93fd089fb7744bf9b27a0662cc48be38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eccb8eddccde497095eeabe05e91d92d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c741664efeb460d804020a73095edf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c448e4d8bced4e3a96968847e3c3b4c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d437fbcc7c0f4c0898a5158723dc510f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac58859332d411c9dfe95b2859df226"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffab85a2efef42c59cf26737f15f1443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b950c55d0a72484ea43f10d59ac6fc70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2c7cede72394f2c8f75f81282ed3ea2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c36dc6d6c77840bebbf0f5d0b4ed42aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e02852ceb2142d88933f57d05dde13c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bde3952ea2934337b5d84b2af72aa74e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"391dba027f594f7f9ee956f01b347136"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# import re\n# from langchain.prompts import PromptTemplate\n# from langchain.chains import LLMChain\n# from langchain.llms import HuggingFacePipeline\n# from transformers import pipeline\n\n# # 1. Enhanced Prompt Template with Examples from PolitiFact / MBFC Style\n# prompt_template = PromptTemplate(\n#     input_variables=[\"title\", \"body\"],\n#     template=(\n#         \"You are an expert in media literacy and critical thinking.\\n\"\n#         \"Your task is to generate 10 investigative questions that a reader should ask to evaluate the reliability, bias, and factual integrity of a news article.\\n\"\n#         \"The questions must:\\n\"\n#         \"- Be at most 300 characters.\\n\"\n#         \"- Avoid compound or overly broad questions.\\n\"\n#         \"- Address potential bias, political slant, source credibility, missing context, and factual support.\\n\"\n#         \"- Focus on identifying misinformation or slant (as PolitiFact and MBFC would).\\n\"\n#         \"- List the most important trust-checking questions first.\\n\\n\"\n\n#         \"Example 1 (PolitiFact style):\\n\"\n#         \"Title: 'Politician Claims Crime is at All-Time High'\\n\"\n#         \"Text: A senator claims that urban crime rates have soared beyond historical levels...\\n\"\n#         \"Questions:\\n\"\n#         \"1. What official crime data is cited to support the claim?\\n\"\n#         \"2. Does the article compare the current rate with past years?\\n\"\n#         \"3. Are crime experts or analysts quoted in the article?\\n\"\n#         \"4. Does the article explain which cities are included?\\n\"\n#         \"5. Is any partisan bias evident in the language?\\n\"\n#         \"6. Are sources properly identified and verifiable?\\n\"\n#         \"7. Are dissenting expert views included?\\n\"\n#         \"8. Are statistics taken from official or third-party sites?\\n\"\n#         \"9. Are terms like 'all-time high' quantified?\\n\"\n#         \"10. Are claims linked to any political motive or bill?\\n\\n\"\n\n#         \"Example 2 (MBFC style):\\n\"\n#         \"Title: 'Website Claims COVID-19 Vaccine Causes Infertility'\\n\"\n#         \"Text: A viral post from XYZ.com alleges the vaccine is linked to infertility...\\n\"\n#         \"Questions:\\n\"\n#         \"1. Is XYZ.com a medically reputable source?\\n\"\n#         \"2. Does the article cite scientific peer-reviewed studies?\\n\"\n#         \"3. Are claims countered by mainstream medical organizations?\\n\"\n#         \"4. Are author credentials provided?\\n\"\n#         \"5. Are experts from both sides of the issue quoted?\\n\"\n#         \"6. Are statistics cherry-picked or taken out of context?\\n\"\n#         \"7. Does the article present evidence-based rebuttals?\\n\"\n#         \"8. Are known misinformation patterns present?\\n\"\n#         \"9. Does the headline reflect the actual content?\\n\"\n#         \"10. Are misleading visuals or graphs used?\\n\\n\"\n\n#         \"Now, given the following article, generate 10 similarly styled questions:\\n\\n\"\n#         \"Title: {title}\\n\\n\"\n#         \"Article text:\\n{body}\\n\\n\"\n#         \"Output only the questions as a numbered list:\\n\"\n#     )\n# )\n\n# # 2. Create model pipeline\n# pipe = pipeline(\n#     \"text-generation\",\n#     model=model,\n#     tokenizer=tokenizer,\n#     max_new_tokens=600,\n#     temperature=0.7,\n#     do_sample=True,\n#     return_full_text=False\n# )\n# llm = HuggingFacePipeline(pipeline=pipe)\n# llm_chain = LLMChain(prompt=prompt_template, llm=llm, output_key=\"text\")\n\n# # 3. Regex to extract clean questions\n# pattern = re.compile(r\"^\\s*\\d+[.)]\\s*(.+)$\")\n\n# # 4. Extraction function\n# def extract_unique_questions(raw_text):\n#     seen = set()\n#     questions = []\n#     for line in raw_text.strip().split(\"\\n\"):\n#         match = pattern.match(line.strip())\n#         if match:\n#             q = match.group(1).strip()\n#             if 0 < len(q) <= 300 and q not in seen:\n#                 seen.add(q)\n#                 questions.append(q)\n#     return questions\n\n# # 5. Main inference loop\n# results = []\n# for idx, topic in enumerate(topics):\n#     docid = topic['docid']\n#     title = topic.get('title', '').strip()\n#     body = topic.get('body', '').strip()[:2000]\n\n#     if not title and not body:\n#         print(f\"‚ö†Ô∏è Skipping empty topic {docid}\")\n#         continue\n\n#     questions = []\n#     for attempt in range(3):\n#         output = llm_chain.invoke({\"title\": title, \"body\": body})\n#         text = output['text'] if isinstance(output, dict) else output\n#         questions = extract_unique_questions(text)\n\n#         if len(questions) >= 10:\n#             questions = questions[:10]\n#             break\n#         else:\n#             print(f\"üîÅ Retry {attempt+1} for {docid} ‚Äî Got {len(questions)} questions\")\n\n#     # Fill short results with 'N/A'\n#     while len(questions) < 10:\n#         questions.append(\"N/A\")\n\n#     results.append({\n#         \"docid\": docid,\n#         \"questions\": questions\n#     })\n\n# # 6. Print examples\n# for item in results[:3]:\n#     print(f\"\\nDocID: {item['docid']}\")\n#     for i, q in enumerate(item['questions'], 1):\n#         print(f\"{i}. {q}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:46:10.000484Z","iopub.execute_input":"2025-08-02T15:46:10.000778Z","iopub.status.idle":"2025-08-02T15:46:10.006645Z","shell.execute_reply.started":"2025-08-02T15:46:10.000758Z","shell.execute_reply":"2025-08-02T15:46:10.006069Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import re\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import pipeline\n\n# 1. Enhanced Prompt Template (TREC-compliant)\nprompt_template = PromptTemplate(\n    input_variables=[\"title\", \"body\"],\n    template=(\n        \"You are a media analyst tasked with creating 10 investigative questions to assess the trustworthiness of a news article.\\n\"\n        \"Each question must:\\n\"\n        \"- Be at most 300 characters\\n\"\n        \"- Focus on one issue only (no compound questions)\\n\"\n        \"- Avoid ambiguity or over-generalization\\n\"\n        \"- Help detect misinformation, bias, political slant, or credibility gaps\\n\\n\"\n        \"Examples (PolitiFact/MBFC style):\\n\"\n        \"Title: 'Politician Claims Crime is at All-Time High'\\n\"\n        \"1. What data backs up the claim about crime rates?\\n\"\n        \"2. Does the article compare this rate to past years?\\n\"\n        \"...\\n\"\n        \"Title: 'Website Claims COVID-19 Vaccine Causes Infertility'\\n\"\n        \"1. Is the source medically reputable?\\n\"\n        \"2. Are scientific studies cited?\\n\"\n        \"...\\n\\n\"\n        \"Now generate 10 questions for the following:\\n\\n\"\n        \"Title: {title}\\n\\n\"\n        \"Article text:\\n{body}\\n\\n\"\n        \"Output only the questions as a numbered list:\\n\"\n    )\n)\n\n# 2. Load LLM\npipe = pipeline(\n    \"text-generation\",\n    model=model,  # assume already loaded\n    tokenizer=tokenizer,  # assume already loaded\n    max_new_tokens=600,\n    temperature=0.7,\n    do_sample=True,\n    return_full_text=False\n)\nllm = HuggingFacePipeline(pipeline=pipe)\nllm_chain = LLMChain(prompt=prompt_template, llm=llm, output_key=\"text\")\n\n# 3. Regex extraction\npattern = re.compile(r\"^\\s*\\d+[.)]\\s*(.+)$\")\n\ndef extract_unique_questions(raw_text):\n    seen = set()\n    questions = []\n    for line in raw_text.strip().split(\"\\n\"):\n        match = pattern.match(line.strip())\n        if match:\n            q = match.group(1).strip()\n            if 0 < len(q) <= 300 and q not in seen:\n                seen.add(q)\n                questions.append(q)\n    return questions\n\n# 4. Inference Loop\nresults = []\nfor topic in topics:\n    docid = topic['docid']\n    title = topic.get('title', '').strip()\n    body = topic.get('body', '').strip()[:2000]\n\n    if not title and not body:\n        print(f\"‚ö†Ô∏è Skipping empty topic {docid}\")\n        continue\n\n    questions = []\n    for attempt in range(3):\n        output = llm_chain.invoke({\"title\": title, \"body\": body})\n        text = output['text'] if isinstance(output, dict) else output\n        questions = extract_unique_questions(text)\n        if len(questions) >= 10:\n            questions = questions[:10]\n            break\n        print(f\"üîÅ Retry {attempt+1} for {docid}: Got {len(questions)}\")\n\n    # Pad with 'N/A' if fewer than 10\n    while len(questions) < 10:\n        questions.append(\"N/A\")\n\n    results.append({\n        \"docid\": docid,\n        \"questions\": questions\n    })\n\n# 5. Print Example Output\nfor item in results[:3]:\n    print(f\"\\nDocID: {item['docid']}\")\n    for i, q in enumerate(item['questions'], 1):\n        print(f\"{i}. {q}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:46:10.008008Z","iopub.execute_input":"2025-08-02T15:46:10.008455Z","iopub.status.idle":"2025-08-02T16:11:50.490734Z","shell.execute_reply.started":"2025-08-02T15:46:10.008437Z","shell.execute_reply":"2025-08-02T16:11:50.490076Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3650943970.py:43: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=pipe)\n/tmp/ipykernel_36/3650943970.py:44: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n  llm_chain = LLMChain(prompt=prompt_template, llm=llm, output_key=\"text\")\n","output_type":"stream"},{"name":"stdout","text":"üîÅ Retry 1 for msmarco_v2.1_doc_34_7751734: Got 2\nüîÅ Retry 2 for msmarco_v2.1_doc_34_7751734: Got 1\nüîÅ Retry 1 for msmarco_v2.1_doc_38_227081897: Got 1\nüîÅ Retry 1 for msmarco_v2.1_doc_39_1165221603: Got 7\nüîÅ Retry 1 for msmarco_v2.1_doc_55_248024230: Got 7\n\nDocID: msmarco_v2.1_doc_04_420132660\n1. ...\n2. Is there verifiable evidence that Sam Panopoulos invented the Hawaiian pizza in 1962?\n3. What historical records confirm the restaurant where the pizza was first created?\n4. Does the article cite sources for the claim that Hawaiian pizza is \"the most insulting\" in the world?\n5. What evidence supports the assertion that the name \"Hawaiian pizza\" has no connection to Hawaii?\n6. Are there documented examples of the pizza being called \"Hawaiian\" before its Canadian origin?\n7. What sources verify the claim that pizza was a \"new dish\" in North America until the 1940s?\n8. Does the article reference studies or surveys about the pizza's popularity or rejection in North America?\n9. What historical data confirms the cheese and tomato pizza version was recent in Naples?\n10. Is there documentation of Sam Panopoulos‚Äôs first exposure to pizza in Naples?\n\nDocID: msmarco_v2.1_doc_06_1440134319\n1. What specific studies link instant noodles to cancer, stroke, or diabetes?\n2. Are the studies cited peer-reviewed and published in reputable journals?\n3. What are the quantities of TBHQ in instant noodles compared to FDA safety limits?\n4. Does the article mention any conflicting studies or expert opinions?\n5. What is the difference between real Japanese Ramen and instant noodles?\n6. How does the article define \"processed\" foods and their health risks?\n7. Are the health risks attributed to instant noodles based on correlation or causation?\n8. What evidence supports the claim that TBHQ causes cancer?\n9. Does the article address the role of overall diet and lifestyle in these health risks?\n10. What is the source of the claim that instant noodles are the number one consumer in China?\n\nDocID: msmarco_v2.1_doc_08_300872161\n1. What legal exemption allows Coca-Cola to import coca leaves?\n2. Are the coca leaves imported by Coca-Cola used in the production of cocaine?\n3. What is the historical basis for the name \"Coca-Cola\"?\n4. Does the article provide evidence of cocaine production from Coca-Cola's coca leaves?\n5. What is the Stepan Company's role in processing coca leaves?\n6. What scientific studies are cited regarding the safety of coca leaves?\n7. What is the U.S. policy on coca leaf importation?\n8. What evidence supports the claim that coca leaves are used to make cocaine?\n9. What is the connection between Coca-Cola's coca leaf imports and cocaine production?\n10. What sources are cited for the claim that Coca-Cola still imports coca leaves?\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\n\nteam_id = \"CUET\"\nrun_id = \"CUET-qwen14B-v2\"\n\nsubmission_rows = []\n\nfor item in results:\n    topic_id = item[\"docid\"]\n    for rank, question in enumerate(item[\"questions\"], 1):\n        # Clean question of unwanted tabs/newlines\n        clean_q = question.replace('\\t', ' ').replace('\\n', ' ').strip()\n        submission_rows.append([\n            topic_id,\n            team_id,\n            run_id,\n            rank,\n            clean_q\n        ])\n\n# Create DataFrame and save\ndf = pd.DataFrame(submission_rows, columns=[\"topic_id\", \"team_id\", \"run_id\", \"rank\", \"question\"])\ndf.to_csv(\"/kaggle/working/CUET_run7.tsv\", sep=\"\\t\", index=False, header=False, encoding=\"utf-8\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:11:50.491442Z","iopub.execute_input":"2025-08-02T16:11:50.491629Z","iopub.status.idle":"2025-08-02T16:11:50.534277Z","shell.execute_reply.started":"2025-08-02T16:11:50.491614Z","shell.execute_reply":"2025-08-02T16:11:50.533734Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# For me","metadata":{}},{"cell_type":"code","source":"\n# === Step 1: Basic Structure Check ===\nprint(f\"‚úÖ Total rows: {len(df)}\")\nprint(f\"‚úÖ Unique topic IDs: {df['topic_id'].nunique()}\")\n\n# === Step 2: Check columns and missing data ===\nif df.isnull().values.any():\n    print(\"‚ùå Found missing values in the file.\")\nelse:\n    print(\"‚úÖ No missing values.\")\n\n# === Step 3: Group by topic ID and check for exactly 10 questions per topic ===\ntopic_counts = df['topic_id'].value_counts()\n\nincorrect_topics = topic_counts[topic_counts != 10]\nif incorrect_topics.empty:\n    print(\"‚úÖ All topic IDs have exactly 10 questions.\")\nelse:\n    print(\"‚ùå The following topic IDs do not have exactly 10 questions:\")\n    print(incorrect_topics)\n\n# === Optional: Check for duplicate questions within a topic ===\ndupes_within_topic = df.duplicated(subset=[\"topic_id\", \"question\"])\nif dupes_within_topic.any():\n    print(\"‚ö†Ô∏è Duplicate questions found within some topics.\")\n    duplicate_rows = df[dupes_within_topic]\n    print(duplicate_rows)\nelse:\n    print(\"‚úÖ No duplicate questions within topics.\")\n\n# === Summary ===\nexpected_rows = df['topic_id'].nunique() * 10\nif len(df) == expected_rows:\n    print(f\"üéØ Submission file has correct total rows: {expected_rows}\")\nelse:\n    print(f\"‚ùå Incorrect number of total rows. Expected: {expected_rows}, Found: {len(df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:11:50.535070Z","iopub.execute_input":"2025-08-02T16:11:50.535334Z","iopub.status.idle":"2025-08-02T16:11:50.571592Z","shell.execute_reply.started":"2025-08-02T16:11:50.535309Z","shell.execute_reply":"2025-08-02T16:11:50.571072Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Total rows: 300\n‚úÖ Unique topic IDs: 30\n‚úÖ No missing values.\n‚úÖ All topic IDs have exactly 10 questions.\n‚úÖ No duplicate questions within topics.\nüéØ Submission file has correct total rows: 300\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Show duplicated questions (exact text matches)\nduplicates = df[df.duplicated('question', keep=False)]\nnum_duplicates = len(duplicates)\n\nprint(f\"üîç Found {num_duplicates} duplicated question rows.\\n\")\nif num_duplicates > 0:\n    print(\"üìÑ Duplicated questions (full rows):\")\n    display(duplicates)\n\n# Frequency count of duplicate questions\nprint(\"\\nüìä Questions that appear more than once:\")\nduplicate_counts = df['question'].value_counts()\ndisplay(duplicate_counts[duplicate_counts > 1])\n\n# Check for 'N/A' values (both literal string and NaN)\nna_mask = df['question'].isna() | (df['question'].str.strip().str.upper() == 'N/A')\nna_rows = df[na_mask]\n\nprint(f\"\\n‚ö†Ô∏è Found {len(na_rows)} rows with 'N/A' or missing question values.\\n\")\nif len(na_rows) > 0:\n    print(\"üö´ Rows with N/A or missing questions:\")\n    display(na_rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:11:50.572383Z","iopub.execute_input":"2025-08-02T16:11:50.572866Z","iopub.status.idle":"2025-08-02T16:11:50.610886Z","shell.execute_reply.started":"2025-08-02T16:11:50.572841Z","shell.execute_reply":"2025-08-02T16:11:50.610363Z"}},"outputs":[{"name":"stdout","text":"üîç Found 2 duplicated question rows.\n\nüìÑ Duplicated questions (full rows):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                           topic_id team_id           run_id  rank question\n0     msmarco_v2.1_doc_04_420132660    CUET  CUET-qwen14B-v2     1      ...\n230  msmarco_v2.1_doc_48_1289995263    CUET  CUET-qwen14B-v2     1      ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>team_id</th>\n      <th>run_id</th>\n      <th>rank</th>\n      <th>question</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>msmarco_v2.1_doc_04_420132660</td>\n      <td>CUET</td>\n      <td>CUET-qwen14B-v2</td>\n      <td>1</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>msmarco_v2.1_doc_48_1289995263</td>\n      <td>CUET</td>\n      <td>CUET-qwen14B-v2</td>\n      <td>1</td>\n      <td>...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nüìä Questions that appear more than once:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"question\n...    2\nName: count, dtype: int64"},"metadata":{}},{"name":"stdout","text":"\n‚ö†Ô∏è Found 0 rows with 'N/A' or missing question values.\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# For changing / regenrating Specific portion question","metadata":{}},{"cell_type":"code","source":"# Format: docid -> list of missing question ranks (1-indexed)\nmissing_questions = {\n    \"msmarco_v2.1_doc_04_420132660\": [1],  # regenerate question 4 and 7\n    \"msmarco_v2.1_doc_48_1289995263\": [1]     # regenerate question 2 only\n}\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:23:54.833615Z","iopub.execute_input":"2025-08-02T16:23:54.833907Z","iopub.status.idle":"2025-08-02T16:23:54.838003Z","shell.execute_reply.started":"2025-08-02T16:23:54.833888Z","shell.execute_reply":"2025-08-02T16:23:54.837178Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"patch_rows = []\n\nfor topic in topics:\n    docid = topic[\"docid\"]\n    if docid not in missing_questions:\n        continue\n\n    title = topic.get('title', '').strip()\n    body = topic.get('body', '').strip()[:2000]\n\n    print(f\"üîç Regenerating for {docid}...\")\n\n    # Generate once\n    for attempt in range(3):\n        output = llm_chain.invoke({\"title\": title, \"body\": body})\n        text = output[\"text\"] if isinstance(output, dict) else output\n        full_questions = extract_unique_questions(text)\n        if len(full_questions) >= 10:\n            break\n        print(f\"Retry {attempt+1}: Got {len(full_questions)}\")\n\n    # If not enough, pad\n    while len(full_questions) < 10:\n        full_questions.append(\"N/A\")\n\n    for rank in missing_questions[docid]:\n        patch_rows.append({\n            \"topic_id\": docid,\n            \"team_id\": \"CUET\",\n            \"run_id\": \"CUET-qwen14B-v2\",\n            \"rank\": rank,\n            \"question\": full_questions[rank - 1]  # rank is 1-based\n        })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:23:57.364560Z","iopub.execute_input":"2025-08-02T16:23:57.364829Z","iopub.status.idle":"2025-08-02T16:25:25.049416Z","shell.execute_reply.started":"2025-08-02T16:23:57.364810Z","shell.execute_reply":"2025-08-02T16:25:25.048785Z"}},"outputs":[{"name":"stdout","text":"üîç Regenerating for msmarco_v2.1_doc_04_420132660...\nüîç Regenerating for msmarco_v2.1_doc_48_1289995263...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\n\n# Define column names manually since submission file has no headers\ncol_names = [\"topic_id\", \"team_id\", \"run_id\", \"rank\", \"question\"]\n\n# Step 1: Load original file (no header in TREC format)\ndf = pd.read_csv(\"/kaggle/working/CUET_run7.tsv\", sep=\"\\t\", header=None, names=col_names)\n\n# Step 2: Convert patch_rows (already in correct format) to DataFrame\npatch_df = pd.DataFrame(patch_rows)\n\n# Step 3: Remove conflicting rows in original submission\nfor i, row in patch_df.iterrows():\n    df = df[~((df['topic_id'] == row['topic_id']) & (df['rank'] == row['rank']))]\n\n# Step 4: Merge and sort\ndf_final = pd.concat([df, patch_df], ignore_index=True).sort_values(by=[\"topic_id\", \"rank\"])\n\n# Step 5: Write back with NO HEADER (TREC requirement)\ndf_final.to_csv(\"/kaggle/working/CUET_run7.tsv\", sep=\"\\t\", index=False, header=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:28:50.957772Z","iopub.execute_input":"2025-08-02T16:28:50.958132Z","iopub.status.idle":"2025-08-02T16:28:50.977420Z","shell.execute_reply.started":"2025-08-02T16:28:50.958107Z","shell.execute_reply":"2025-08-02T16:28:50.976642Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Show duplicated questions (exact text matches)\nduplicates = df[df.duplicated('question', keep=False)]\nnum_duplicates = len(duplicates)\n\nprint(f\"üîç Found {num_duplicates} duplicated question rows.\\n\")\nif num_duplicates > 0:\n    print(\"üìÑ Duplicated questions (full rows):\")\n    display(duplicates)\n\n# Frequency count of duplicate questions\nprint(\"\\nüìä Questions that appear more than once:\")\nduplicate_counts = df['question'].value_counts()\ndisplay(duplicate_counts[duplicate_counts > 1])\n\n# Check for 'N/A' values (both literal string and NaN)\nna_mask = df['question'].isna() | (df['question'].str.strip().str.upper() == 'N/A')\nna_rows = df[na_mask]\n\nprint(f\"\\n‚ö†Ô∏è Found {len(na_rows)} rows with 'N/A' or missing question values.\\n\")\nif len(na_rows) > 0:\n    print(\"üö´ Rows with N/A or missing questions:\")\n    display(na_rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:28:58.289294Z","iopub.execute_input":"2025-08-02T16:28:58.289574Z","iopub.status.idle":"2025-08-02T16:28:58.301235Z","shell.execute_reply.started":"2025-08-02T16:28:58.289553Z","shell.execute_reply":"2025-08-02T16:28:58.300498Z"}},"outputs":[{"name":"stdout","text":"üîç Found 0 duplicated question rows.\n\n\nüìä Questions that appear more than once:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Series([], Name: count, dtype: int64)"},"metadata":{}},{"name":"stdout","text":"\n‚ö†Ô∏è Found 0 rows with 'N/A' or missing question values.\n\n","output_type":"stream"}],"execution_count":14}]}