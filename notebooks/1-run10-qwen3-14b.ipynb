{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12443031,"sourceType":"datasetVersion","datasetId":7849167}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:41:54.210083Z","iopub.execute_input":"2025-08-08T11:41:54.210347Z","iopub.status.idle":"2025-08-08T11:41:54.490409Z","shell.execute_reply.started":"2025-08-08T11:41:54.210326Z","shell.execute_reply":"2025-08-08T11:41:54.489659Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/trec-2025/2025-starter-kit-main/.example.env\n/kaggle/input/trec-2025/2025-starter-kit-main/main.py\n/kaggle/input/trec-2025/2025-starter-kit-main/README.md\n/kaggle/input/trec-2025/2025-starter-kit-main/produce_run.py\n/kaggle/input/trec-2025/2025-starter-kit-main/output/tracking_data.json\n/kaggle/input/trec-2025/2025-starter-kit-main/output/dragun-organizers-starter-kit-task-1\n/kaggle/input/trec-2025/2025-starter-kit-main/output/dragun-organizers-starter-kit-task-2\n/kaggle/input/trec-2025/2025-starter-kit-main/data/trec-2025-dragun-topics.jsonl\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/report_generator.py\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/segment_retriever.py\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/question_generator.py\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/query_generator.py\n/kaggle/input/trec-2025/2025-starter-kit-main/modules/information_evaluator.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n    !pip install transformers==4.51.3\n    !pip install --no-deps unsloth\n    !pip install langchain langchain-community\n    !pip install jsonlines\n    !pip install --quiet langchain transformers sentencepiece\n\nimport pandas as pd\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import LLMChain\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nimport json\nfrom unsloth import FastLanguageModel\nfrom unsloth import FastModel\nimport torch\nfrom tqdm import tqdm\nfrom unsloth.chat_templates import get_chat_template","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:41:54.491895Z","iopub.execute_input":"2025-08-08T11:41:54.492197Z","iopub.status.idle":"2025-08-08T11:43:08.423494Z","shell.execute_reply.started":"2025-08-08T11:41:54.492178Z","shell.execute_reply":"2025-08-08T11:43:08.422872Z"}},"outputs":[{"name":"stderr","text":"2025-08-08 11:42:42.191516: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754653362.367966      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754653362.425006      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"TOPICS_PATH = '/kaggle/input/trec-2025/2025-starter-kit-main/data/trec-2025-dragun-topics.jsonl'\n\ndef load_jsonl(fname):\n    with open(fname, 'r', encoding='utf-8') as f:\n        return [json.loads(line) for line in f]\ntopics = load_jsonl(TOPICS_PATH)\nprint(\"Loaded\", len(topics), \"topics.\")\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)  \n\nfrom langchain.prompts import PromptTemplate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:43:08.424294Z","iopub.execute_input":"2025-08-08T11:43:08.424574Z","iopub.status.idle":"2025-08-08T11:43:57.077175Z","shell.execute_reply.started":"2025-08-08T11:43:08.424541Z","shell.execute_reply":"2025-08-08T11:43:57.076573Z"}},"outputs":[{"name":"stdout","text":"Loaded 30 topics.\n==((====))==  Unsloth 2025.8.1: Fast Qwen3 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"895b0d0ff8c546bda4d921dd96d46f62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9928f4890bb24c1896f0cd235985bcd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c190c95887cf49d3a20a85d4e2011a12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06d4c8e4a9f487a9abf290525070581"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27200d6d61ad4582b6d7960766167073"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2908049b241e48a59a3e6317f88df310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b0779fbc8b4b0e8d7fc440ed257ac7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b77dbd33ad14d7bac93cd58e12b4c86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cb5de08190e4a539dc11b651d93f507"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae643585095f4d4397126c7f7be86987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb9589a5d6a416b839f9a8abea4745a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22709f0e3dae462ab58779fbe155d42d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e57b8504b5f74337a87832c7db9fc258"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de52ceca4494bbd8c5e253f7f46e45b"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"prompt_template = PromptTemplate(\n    input_variables=[\"title\", \"body\"],\n    template=(\n        \"\"\"You are a professional fact-checker and investigative media literacy expert, trained on resources like PolitiFact, MBFC, NewsQA, and FEVER. \nYour task: Generate 10 sharp, critical, and investigative questions a thoughtful reader should ask to assess the trustworthiness of a given news article. \nThese must be specific to the article, ranked from most to least important for evaluating credibility.\n\nCore focus areas for question generation:\n1. Investigate the Source:\n   - Publisher's or quoted sources' background, reputation, and biases.\n   - Ownership, funding, and editorial policies.\n   - Credentials and past track record.\n\n2. Examine Claims and Evidence:\n   - Central factual assertions and their reliability.\n   - Quality of supporting evidence or missing context.\n   - Alternative explanations not mentioned.\n\n3. Trace Information Origins:\n   - Original sources for quotes, data, or statistics.\n   - Methodology of cited research.\n   - How information flows from primary to secondary sources.\n\n4. Assess Perspective and Balance:\n   - Missing viewpoints or underrepresented perspectives.\n   - Diversity of expert opinions.\n\n5. Evaluate Context and Timing:\n   - Broader events surrounding the story.\n   - Potential motivations for timing.\n   - Related developments that might shift interpretation.\n\nGuidelines for each question:\n- Max 300 characters.\n- One question per idea (no compound questions).\n- Avoid vague/general phrasing.\n- Avoid questions fully answerable from the article itself; require external verification or deeper inquiry.\n- Be actionable for a reader to investigate.\n\n---\n\nExample 1 (PolitiFact-style):\nTitle: \"WHO: Processed meat causes cancer\"\nArticle: WHO classified processed meat as carcinogenic...\nQuestions:\n1. What peer-reviewed studies support WHO's claim?\n2. Is the research methodology disclosed in the article?\n3. Who funded the studies mentioned?\n4. Are dissenting expert views included?\n5. Are the health risks exaggerated?\n6. Is primary data linked or cited?\n7. Does the article mention affected demographics?\n8. Are any advocacy groups quoted?\n9. Is causal language justified?\n10. Are the claims corroborated by other agencies?\n\n---\n\nExample 2 (MBFC-style):\nTitle: \"CDC says masks are optional outdoors\"\nArticle: CDC updates its outdoor masking guidance...\nQuestions:\n1. Is there scientific consensus supporting this change?\n2. Does the article quote data or just opinion?\n3. Are counterarguments or dissenting experts cited?\n4. Are political motives or timing relevant?\n5. Is this consistent with international guidance?\n6. Does the article omit relevant prior CDC guidance?\n7. Are potential risks understated?\n8. Does the article rely on official or third-party data?\n9. Are specific regional risks acknowledged?\n10. Is the framing objective or emotionally loaded?\n\n---\n\nNow, using the same investigative approach, write 10 questions for this article:\n\nTitle: {title}\nArticle text:\n{body}\n\nOutput only the questions as a numbered list from 1 to 10, each ending with a question mark.\n\"\"\"\n    )\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:43:57.077931Z","iopub.execute_input":"2025-08-08T11:43:57.078163Z","iopub.status.idle":"2025-08-08T11:43:57.083484Z","shell.execute_reply.started":"2025-08-08T11:43:57.078145Z","shell.execute_reply":"2025-08-08T11:43:57.082955Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import re\n\n# 2. Load LLM\npipe = pipeline(\n    \"text-generation\",\n    model=model,  # assume already loaded\n    tokenizer=tokenizer,  # assume already loaded\n    max_new_tokens=600,\n    temperature=0.6,\n    top_p=0.9,\n    do_sample=True,\n    return_full_text=False\n)\nllm = HuggingFacePipeline(pipeline=pipe)\nllm_chain = LLMChain(prompt=prompt_template, llm=llm, output_key=\"text\")\n\n# 3. Regex extraction\npattern = re.compile(r\"^\\s*\\d+[.)]\\s*(.+)$\")\n\ndef extract_unique_questions(raw_text):\n    seen = set()\n    questions = []\n    for line in raw_text.strip().split(\"\\n\"):\n        match = pattern.match(line.strip())\n        if match:\n            q = match.group(1).strip()\n            if 0 < len(q) <= 300 and q not in seen:\n                seen.add(q)\n                questions.append(q)\n    return questions\n\n# 4. Inference Loop\nresults = []\nfor topic in topics:\n    docid = topic['docid']\n    title = topic.get('title', '').strip()\n    body = topic.get('body', '').strip()[:2000]\n\n    if not title and not body:\n        print(f\"‚ö†Ô∏è Skipping empty topic {docid}\")\n        continue\n\n    questions = []\n    for attempt in range(3):\n        output = llm_chain.invoke({\"title\": title, \"body\": body})\n        text = output['text'] if isinstance(output, dict) else output\n        questions = extract_unique_questions(text)\n        if len(questions) >= 10:\n            questions = questions[:10]\n            break\n        print(f\"üîÅ Retry {attempt+1} for {docid}: Got {len(questions)}\")\n\n    # Pad with 'N/A' if fewer than 10\n    while len(questions) < 10:\n        questions.append(\"N/A\")\n\n    results.append({\n        \"docid\": docid,\n        \"questions\": questions\n    })\n\n# 5. Print Example Output\nfor item in results[:3]:\n    print(f\"\\nDocID: {item['docid']}\")\n    for i, q in enumerate(item['questions'], 1):\n        print(f\"{i}. {q}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:43:57.085137Z","iopub.execute_input":"2025-08-08T11:43:57.085389Z","iopub.status.idle":"2025-08-08T12:10:40.948111Z","shell.execute_reply.started":"2025-08-08T11:43:57.085373Z","shell.execute_reply":"2025-08-08T12:10:40.947355Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/701901908.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=pipe)\n/tmp/ipykernel_36/701901908.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n  llm_chain = LLMChain(prompt=prompt_template, llm=llm, output_key=\"text\")\n","output_type":"stream"},{"name":"stdout","text":"üîÅ Retry 1 for msmarco_v2.1_doc_21_861891150: Got 6\nüîÅ Retry 2 for msmarco_v2.1_doc_21_861891150: Got 2\nüîÅ Retry 3 for msmarco_v2.1_doc_21_861891150: Got 9\nüîÅ Retry 1 for msmarco_v2.1_doc_25_501708725: Got 6\n\nDocID: msmarco_v2.1_doc_04_420132660\n1. What specific historical records or documents confirm Sam Panopoulos as the inventor of the Hawaiian pizza?\n2. Are there any credible sources or contemporaneous accounts from the 1960s that corroborate the claim of the pizza's creation in London, Ontario?\n3. How does the article address the possibility that other chefs or restaurants may have independently created a similar dish around the same time?\n4. What evidence supports the assertion that the Hawaiian pizza originated in Ontario rather than Hawaii, and how was this conclusion reached?\n5. What is the methodology used to determine that the pizza was created in 1962, and are there any primary sources from that year?\n6. What is the role of the Naples stop during Sam Panopoulos's journey in influencing the creation of the Hawaiian pizza?\n7. Are there any academic or culinary historians cited in the article who validate the narrative of the pizza's Canadian origin?\n8. How does the article explain the discrepancy between the pizza's name and its actual geographical origin?\n9. What are the specific reasons cited for the intense dislike of Hawaiian pizza among some consumers, and are these reasons supported by surveys or studies?\n10. What is the connection between the article's publication on Gastro Obscura and any potential biases in presenting the history of the Hawaiian pizza?\n\nDocID: msmarco_v2.1_doc_06_1440134319\n1. What peer-reviewed studies link instant noodles to stroke, diabetes, and cancer?\n2. What is the specific methodology of the studies cited in the article?\n3. Who funded the research mentioned in the article?\n4. Are there dissenting expert opinions or studies that contradict the claims?\n5. Is the amount of TBHQ in instant noodles sufficient to pose a health risk?\n6. Are the health risks of TBHQ supported by regulatory agencies like the FDA or WHO?\n7. Does the article provide data on the actual consumption levels of instant noodles in the populations studied?\n8. How does the article differentiate between real Japanese Ramen and instant noodles in terms of health impact?\n9. Are the claims about instant noodles' health risks corroborated by other health organizations or studies?\n10. What is the role of the author's background as a nutritionist and medical writer in shaping the article's perspective?\n\nDocID: msmarco_v2.1_doc_08_300872161\n1. What is the legal basis for Coca-Cola's exemption from U.S. laws against coca leaf imports?\n2. What peer-reviewed studies confirm the safety of coca leaves in their natural form?\n3. Who funds NaturalNews.com, and what is their editorial policy on health and pharmaceutical topics?\n4. What is the Stepan Company's role in processing coca leaves, and are they regulated by U.S. agencies?\n5. Are there alternative explanations for the claim that coca leaves are used to manufacture cocaine in the U.S.?\n6. What specific data or sources does the article cite to support the assertion that Coca-Cola still imports coca leaves?\n7. How does the article address the historical context of cocaine in Coca-Cola's original formula?\n8. What are the U.S. government's stated reasons for allowing Coca-Cola's exemption from coca leaf import laws?\n9. Are there other U.S. corporations permitted to legally import coca leaves, or is Coca-Cola uniquely exempt?\n10. What is the relationship between the Andean coca leaf trade and U.S. foreign policy in South America?\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\nteam_id = \"CUET\"\nrun_id = \"CUET-qwen14B-v5\"\n\nsubmission_rows = []\n\nfor item in results:\n    topic_id = item[\"docid\"]\n    for rank, question in enumerate(item[\"questions\"], 1):\n        # Clean question of unwanted tabs/newlines\n        clean_q = question.replace('\\t', ' ').replace('\\n', ' ').strip()\n        submission_rows.append([\n            topic_id,\n            team_id,\n            run_id,\n            rank,\n            clean_q\n        ])\n\n# Create DataFrame and save\ndf = pd.DataFrame(submission_rows, columns=[\"topic_id\", \"team_id\", \"run_id\", \"rank\", \"question\"])\ndf.to_csv(\"/kaggle/working/CUET_run10.tsv\", sep=\"\\t\", index=False, header=False, encoding=\"utf-8\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:10:40.948769Z","iopub.execute_input":"2025-08-08T12:10:40.948956Z","iopub.status.idle":"2025-08-08T12:10:40.973235Z","shell.execute_reply.started":"2025-08-08T12:10:40.948941Z","shell.execute_reply":"2025-08-08T12:10:40.972459Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Show duplicated questions (exact text matches)\nduplicates = df[df.duplicated('question', keep=False)]\nnum_duplicates = len(duplicates)\n\nprint(f\"üîç Found {num_duplicates} duplicated question rows.\\n\")\nif num_duplicates > 0:\n    print(\"üìÑ Duplicated questions (full rows):\")\n    display(duplicates)\n\n# Frequency count of duplicate questions\nprint(\"\\nüìä Questions that appear more than once:\")\nduplicate_counts = df['question'].value_counts()\ndisplay(duplicate_counts[duplicate_counts > 1])\n\n# Check for 'N/A' values (both literal string and NaN)\nna_mask = df['question'].isna() | (df['question'].str.strip().str.upper() == 'N/A')\nna_rows = df[na_mask]\n\nprint(f\"\\n‚ö†Ô∏è Found {len(na_rows)} rows with 'N/A' or missing question values.\\n\")\nif len(na_rows) > 0:\n    print(\"üö´ Rows with N/A or missing questions:\")\n    display(na_rows)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:10:40.974187Z","iopub.execute_input":"2025-08-08T12:10:40.974402Z","iopub.status.idle":"2025-08-08T12:10:41.012799Z","shell.execute_reply.started":"2025-08-08T12:10:40.974386Z","shell.execute_reply":"2025-08-08T12:10:41.012152Z"}},"outputs":[{"name":"stdout","text":"üîç Found 0 duplicated question rows.\n\n\nüìä Questions that appear more than once:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Series([], Name: count, dtype: int64)"},"metadata":{}},{"name":"stdout","text":"\n‚ö†Ô∏è Found 1 rows with 'N/A' or missing question values.\n\nüö´ Rows with N/A or missing questions:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                         topic_id team_id           run_id  rank question\n49  msmarco_v2.1_doc_21_861891150    CUET  CUET-qwen14B-v5    10      N/A","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>team_id</th>\n      <th>run_id</th>\n      <th>rank</th>\n      <th>question</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>49</th>\n      <td>msmarco_v2.1_doc_21_861891150</td>\n      <td>CUET</td>\n      <td>CUET-qwen14B-v5</td>\n      <td>10</td>\n      <td>N/A</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Step 2: Create dict of missing ranks per topic_id\nmissing_questions = {}\nfor _, row in na_rows.iterrows():\n    topic_id = row[\"topic_id\"]\n    rank = int(row[\"rank\"])\n    if topic_id not in missing_questions:\n        missing_questions[topic_id] = []\n    missing_questions[topic_id].append(rank)\n\n# Step 3: Load topics.jsonl into dict\ntopic_path = \"/kaggle/input/trec-2025/2025-starter-kit-main/data/trec-2025-dragun-topics.jsonl\"\ntopics_dict = {}\nwith open(topic_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        obj = json.loads(line)\n        docid = obj.get(\"docid\", \"\").strip()\n        topics_dict[docid] = {\n            \"title\": obj.get(\"title\", \"\"),\n            \"body\": obj.get(\"body\", \"\")\n        }\n\n# Step 4: Regenerate only missing ranks\npatch_rows = []\n\nfor docid, ranks in missing_questions.items():\n    if docid not in topics_dict:\n        print(f\"‚ùå Topic ID {docid} not found in topics.jsonl, skipping.\")\n        continue\n\n    title = topics_dict[docid][\"title\"]\n    body = topics_dict[docid][\"body\"][:2000]\n\n    print(f\"üîÅ Regenerating for topic {docid}...\")\n\n    full_questions = []\n    for attempt in range(3):\n        output = llm_chain.invoke({\"title\": title, \"body\": body})\n        text = output[\"text\"] if isinstance(output, dict) else output\n        full_questions = extract_unique_questions(text)\n        if len(full_questions) >= 10:\n            break\n        print(f\"  Retry {attempt + 1}: Only got {len(full_questions)}\")\n\n    # Pad to 10 if still short\n    while len(full_questions) < 10:\n        full_questions.append(\"N/A\")\n\n    # Add only the requested ranks\n    for rank in ranks:\n        patch_rows.append({\n            \"topic_id\": docid,\n            \"team_id\": \"CUET\",\n            \"run_id\": \"CUET-qwen14B-v5\",\n            \"rank\": rank,\n            \"question\": full_questions[rank - 1]\n        })\n\n# Step 5: Patch into original DataFrame\npatch_df = pd.DataFrame(patch_rows)\n\n# Remove N/A rows\ndf = df[~na_mask]\n\n# Combine and sort\ndf_final = pd.concat([df, patch_df], ignore_index=True).sort_values(by=[\"topic_id\", \"rank\"])\n\n# Step 6: Save without header\ndf_final.to_csv(\"/kaggle/working/CUET_run10_fixed.tsv\", sep=\"\\t\", index=False, header=False)\n# Vule run9 hishebe save kore felsi and run 10 hobe\n\nprint(f\"\\n‚úÖ Regenerated and patched {len(patch_rows)} N/A questions.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:10:41.013482Z","iopub.execute_input":"2025-08-08T12:10:41.013792Z","iopub.status.idle":"2025-08-08T12:11:27.650704Z","shell.execute_reply.started":"2025-08-08T12:10:41.013774Z","shell.execute_reply":"2025-08-08T12:11:27.649928Z"}},"outputs":[{"name":"stdout","text":"üîÅ Regenerating for topic msmarco_v2.1_doc_21_861891150...\n\n‚úÖ Regenerated and patched 1 N/A questions.\n","output_type":"stream"}],"execution_count":8}]}